{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bio.csv', sep=',')\n",
    "data = data.replace(np.nan, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data['t'].to_numpy().reshape(-1,1)\n",
    "cB = data['cB'].to_numpy().reshape(-1,1)\n",
    "cTG = data['cTG'].to_numpy().reshape(-1,1)\n",
    "cDG = data['cDG'].to_numpy().reshape(-1,1)\n",
    "cMG = data['cMG'].to_numpy().reshape(-1,1)\n",
    "cG = data['cG'].to_numpy().reshape(-1,1)\n",
    "c = np.concatenate((cB, cTG, cDG, cMG, cG), axis=1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_points = 4\n",
    "\n",
    "t_train = np.linspace(t[0], t[-1], total_points)\n",
    "\n",
    "idx = []\n",
    "for ti in t:\n",
    "    idx.append(np.where(t_train.reshape(-1,1) == ti)[0][0])\n",
    "idx = np.array(idx)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cTG_train = cTG\n",
    "cDG_train = cDG\n",
    "cMG_train = cMG\n",
    "cTG_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = torch.from_numpy(t_train).float().to(device)\n",
    "cTG_train = torch.from_numpy(cTG_train).float().to(device)\n",
    "cDG_train = torch.from_numpy(cDG_train).float().to(device)\n",
    "cMG_train = torch.from_numpy(cMG_train).float().to(device)\n",
    "cTG_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 1.\n",
    "k2 = 1.\n",
    "k3 = 1.\n",
    "k4 = 1.\n",
    "k5 = 1.\n",
    "k6 = 1.\n",
    "\n",
    "layers = np.array([1,10,10,10,1])\n",
    "\n",
    "f_hat = torch.zeros(t_train.shape[0],1).to(device)\n",
    "\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "     \n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            \n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "    def forward(self, x):\n",
    "              \n",
    "        if torch.is_tensor(x) != True:\n",
    "            x = torch.from_numpy(x)\n",
    "        \n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = self.linears[i](a)\n",
    "                        \n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a1 = self.linears[-1](a)\n",
    "        a2 = self.linears[-1](a)\n",
    "        a3 = self.linears[-1](a)\n",
    "        a4 = self.linears[-1](a)\n",
    "        a5 = self.linears[-1](a)\n",
    "        \n",
    "        return a1, a2, a3, a4, a5\n",
    "\n",
    "DNN(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN():\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.loss_function = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "        self.iter = 0\n",
    "\n",
    "        self.k1 = torch.tensor([k1], requires_grad=True).float().to(device)\n",
    "        self.k2 = torch.tensor([k2], requires_grad=True).float().to(device)\n",
    "        self.k3 = torch.tensor([k3], requires_grad=True).float().to(device)\n",
    "        self.k4 = torch.tensor([k4], requires_grad=True).float().to(device)\n",
    "        self.k5 = torch.tensor([k5], requires_grad=True).float().to(device)\n",
    "        self.k6 = torch.tensor([k6], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.k1 = nn.Parameter(self.k1)\n",
    "        self.k2 = nn.Parameter(self.k2)\n",
    "        self.k3 = nn.Parameter(self.k3)\n",
    "        self.k4 = nn.Parameter(self.k4)\n",
    "        self.k5 = nn.Parameter(self.k5)\n",
    "        self.k6 = nn.Parameter(self.k6)\n",
    "\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        self.dnn.register_parameter('k1', self.k1)\n",
    "        self.dnn.register_parameter('k2', self.k2)\n",
    "        self.dnn.register_parameter('k3', self.k3)\n",
    "        self.dnn.register_parameter('k4', self.k4)\n",
    "        self.dnn.register_parameter('k5', self.k5)\n",
    "        self.dnn.register_parameter('k6', self.k6)\n",
    "        \n",
    "    def loss_cB(self, x):\n",
    "        \n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        cB, cTG, cDG, cMG, cG = self.dnn(g)\n",
    "        \n",
    "        grad_cB = autograd.grad(cB, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cB_ode = self.loss_function(grad_cB - self.k1*cTG + self.k2*cDG*cB \\\n",
    "                                                 - self.k3*cDG + self.k4*cMG*cB \\\n",
    "                                                 - self.k5*cMG + self.k6*cG*cB, f_hat)\n",
    "        \n",
    "        return loss_cB_ode\n",
    "    \n",
    "    def loss_cTG(self, x, y):\n",
    "        \n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        cB, cTG, cDG, _, _ = self.dnn(g)\n",
    "        \n",
    "        grad_cTG = autograd.grad(cTG, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cTG_ode = self.loss_function(grad_cTG + self.k1*cTG - self.k2*cDG*cB, f_hat)\n",
    "        \n",
    "        loss_cTG_data = self.loss_function(cTG, y)\n",
    "        \n",
    "        return alpha*loss_cTG_ode + (1-alpha)*loss_cTG_data\n",
    "    \n",
    "    def loss_cDG(self, x, y):\n",
    "        \n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        cB, cTG, cDG, cMG, _ = self.dnn(g)\n",
    "        \n",
    "        grad_cDG = autograd.grad(cDG, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cDG_ode = self.loss_function(grad_cDG - self.k1*cTG + self.k2*cDG*cB \\\n",
    "                                                   + self.k3*cDG - self.k4*cMG*cB, f_hat)\n",
    "        \n",
    "        loss_cDG_data = self.loss_function(cDG, y)\n",
    "        \n",
    "        return alpha*loss_cDG_ode + (1-alpha)*loss_cDG_data\n",
    "    \n",
    "    def loss_cMG(self, x, y):\n",
    "        \n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        cB, _, cDG, cMG, cG = self.dnn(g)\n",
    "        \n",
    "        grad_cMG = autograd.grad(cMG, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cMG_ode = self.loss_function(grad_cMG - self.k3*cDG + self.k4*cMG*cB \\\n",
    "                                                   + self.k5*cMG - self.k6*cG*cB, f_hat)\n",
    "        \n",
    "        loss_cMG_data = self.loss_function(cMG, y)\n",
    "        \n",
    "        return alpha*loss_cMG_ode + (1-alpha)*loss_cMG_data\n",
    "    \n",
    "    def loss_cG(self, x):\n",
    "        \n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        cB, _, _, cMG, cG = self.dnn(g)\n",
    "        \n",
    "        grad_cG = autograd.grad(cG, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cG_ode = self.loss_function(grad_cG - self.k5*cMG + self.k6*cG*cB, f_hat)\n",
    "                \n",
    "        return loss_cG_ode\n",
    "    \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_u1 = self.loss_cB(t_train)\n",
    "        loss_u2 = self.loss_cTG(t_train, cTG_train)\n",
    "        loss_u3 = self.loss_cDG(t_train, cDG_train)\n",
    "        loss_u4 = self.loss_cMG(t_train, cMG_train)\n",
    "        loss_u5 = self.loss_cG(t_train)\n",
    "        \n",
    "        loss = loss_u1 + loss_u2 + loss_u3 + loss_u4 + loss_u5\n",
    "        \n",
    "        loss.backward()\n",
    "                \n",
    "        self.iter += 1\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN = FCN(layers)\n",
    "\n",
    "PINN.dnn(t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(PINN.dnn.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=1)\n",
    "\n",
    "while PINN.iter < 100:\n",
    "    optimizer.step(PINN.closure)\n",
    "    for p in PINN.dnn.parameters():\n",
    "        p.data.clamp_(min=0.)\n",
    "        \n",
    "print(PINN.dnn(t_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDO(y, prm):\n",
    "    \n",
    "    k1 = prm[0]\n",
    "    k2 = prm[1]\n",
    "    k3 = prm[2]\n",
    "    k4 = prm[3]\n",
    "    k5 = prm[4]\n",
    "    k6 = prm[5]\n",
    "    \n",
    "    f = np.zeros(5)\n",
    "    \n",
    "    f[1] = - k1*y[1] + k2*y[2]*y[0]\n",
    "    f[2] = + k1*y[1] - k2*y[2]*y[0] - k3*y[2] + k4*y[3]*y[0]\n",
    "    f[3] = + k3*y[2] - k4*y[3]*y[0] - k5*y[3] + k6*y[4]*y[0]\n",
    "    f[4] = + k5*y[3] - k6*y[4]*y[0]\n",
    "    f[0] = + k1*y[1] - k2*y[2]*y[0] + k3*y[2] - k4*y[3]*y[0] + k5*y[3] - k6*y[4]*y[0]\n",
    "    \n",
    "    return f\n",
    "\n",
    "def euler_explicite(y0, dt, tf, prm):\n",
    "    \n",
    "    mat_y = np.array([y0])\n",
    "    \n",
    "    t = np.array([0])\n",
    "    while t[-1] < tf:\n",
    "        y = y0 + dt * EDO(y0, prm)\n",
    "        \n",
    "        mat_y = np.append(mat_y, [y], axis=0)\n",
    "        t = np.append(t, t[-1]+dt)\n",
    "        \n",
    "        y0 = np.copy(y)\n",
    "    \n",
    "    return t, mat_y\n",
    "\n",
    "prm = []        \n",
    "for i in range(6):\n",
    "    prm.append(params[:6][i][0].cpu().detach().numpy())\n",
    "    \n",
    "t_euler, y_euler = euler_explicite(np.array([0,0.540121748,0.057018273,0,0]), 0.001, 6, prm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,0])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[0].detach().numpy())\n",
    "plt.plot(t, cB, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,1])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[1].detach().numpy())\n",
    "plt.plot(t, cTG, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,2])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[2].detach().numpy())\n",
    "plt.plot(t, cDG, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,3])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[3].detach().numpy())\n",
    "plt.plot(t, cMG, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,4])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[4].detach().numpy())\n",
    "plt.plot(t, cG, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f538c6a538005cf3b39e004aac3ceb405c590ed47051b2fc8a4226903742b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
