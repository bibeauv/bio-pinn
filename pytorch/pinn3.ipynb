{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bio.csv', sep=',')\n",
    "data = data.replace(np.nan, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data['t'].to_numpy().reshape(-1,1)\n",
    "cB = data['cB'].to_numpy().reshape(-1,1)\n",
    "cTG = data['cTG'].to_numpy().reshape(-1,1)\n",
    "cDG = data['cDG'].to_numpy().reshape(-1,1)\n",
    "cMG = data['cMG'].to_numpy().reshape(-1,1)\n",
    "cG = data['cG'].to_numpy().reshape(-1,1)\n",
    "c = np.concatenate((cB, cTG, cDG, cMG, cG), axis=1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_points = 4\n",
    "\n",
    "t_train = np.linspace(t[0], t[-1], total_points)\n",
    "\n",
    "idx = []\n",
    "for ti in t:\n",
    "    idx.append(np.where(t_train.reshape(-1,1) == ti)[0][0])\n",
    "idx = np.array(idx)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cB_train = cB\n",
    "cTG_train = cTG\n",
    "cDG_train = cDG\n",
    "cMG_train = cMG\n",
    "cG_train = cG\n",
    "cTG_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = torch.from_numpy(t_train).float().to(device)\n",
    "cB_train = torch.from_numpy(cB_train).float().to(device)\n",
    "cTG_train = torch.from_numpy(cTG_train).float().to(device)\n",
    "cDG_train = torch.from_numpy(cDG_train).float().to(device)\n",
    "cMG_train = torch.from_numpy(cMG_train).float().to(device)\n",
    "cG_train = torch.from_numpy(cG_train).float().to(device)\n",
    "cTG_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 1.\n",
    "k2 = 1.\n",
    "k3 = 1.\n",
    "k4 = 1.\n",
    "k5 = 1.\n",
    "k6 = 1.\n",
    "k = [k1,k2,k3,k4,k5,k6]\n",
    "\n",
    "layers = np.array([1,10,10,10,1])\n",
    "\n",
    "f_hat = torch.zeros(t_train.shape[0],1).to(device)\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "c_pred = torch.cat((cB_train, cTG_train, cDG_train, cMG_train, cG_train), 1)\n",
    "c_pred = c_pred.to(device)\n",
    "c_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        self.f1 = nn.Linear(1, 10)\n",
    "        self.f2 = nn.Linear(10, 10)\n",
    "        self.f3 = nn.Linear(10, 10)\n",
    "        self.out = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if torch.is_tensor(x) != True:\n",
    "            x = torch.from_numpy(x)\n",
    "        \n",
    "        a = x.clone().float()\n",
    "        \n",
    "        z_1 = self.f1(a)\n",
    "        a_1 = self.activation(z_1)\n",
    "        z_2 = self.f2(a_1)\n",
    "        a_2 = self.activation(z_2)\n",
    "        z_3 = self.f3(a_2)\n",
    "        a_3 = self.activation(z_3)\n",
    "        z_4 = self.out(a_3)\n",
    "        a_4 = z_4.clone()\n",
    "        \n",
    "        return a_4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN Triglycerides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cTGNN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dnn = subNN().to(device)\n",
    "        self.loss_function = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "        self.k1 = torch.tensor([k1], requires_grad=True).float().to(device)\n",
    "        self.k2 = torch.tensor([k2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.k1 = nn.Parameter(self.k1)\n",
    "        self.k2 = nn.Parameter(self.k2)\n",
    "        \n",
    "        self.dnn.register_parameter('k1', self.k1)\n",
    "        self.dnn.register_parameter('k2', self.k2)\n",
    "        \n",
    "    def pred(self, c_pred):\n",
    "        self.cB = c_pred[:,0].reshape(-1,1)\n",
    "        self.cDG = c_pred[:,2].reshape(-1,1)\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        self.cTG = self.dnn(g)\n",
    "        \n",
    "        grad_cTG = autograd.grad(self.cTG, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cTG_ode = self.loss_function(grad_cTG + self.k1*self.cTG - self.k2*self.cDG*self.cB, f_hat)\n",
    "        \n",
    "        loss_cTG_data = self.loss_function(self.cTG, y)\n",
    "        \n",
    "        loss = alpha*loss_cTG_ode + (1-alpha)*loss_cTG_data\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        self.pred(c_pred)\n",
    "        \n",
    "        loss = self.loss(t_train, cTG_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "cTGNN().dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN Diglycerides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cDGNN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dnn = subNN().to(device)\n",
    "        self.loss_function = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "        self.k1 = torch.tensor([k1], requires_grad=True).float().to(device)\n",
    "        self.k2 = torch.tensor([k2], requires_grad=True).float().to(device)\n",
    "        self.k3 = torch.tensor([k3], requires_grad=True).float().to(device)\n",
    "        self.k4 = torch.tensor([k4], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.k1 = nn.Parameter(self.k1)\n",
    "        self.k2 = nn.Parameter(self.k2)\n",
    "        self.k3 = nn.Parameter(self.k3)\n",
    "        self.k4 = nn.Parameter(self.k4)\n",
    "        \n",
    "        self.dnn.register_parameter('k1', self.k1)\n",
    "        self.dnn.register_parameter('k2', self.k2)\n",
    "        self.dnn.register_parameter('k3', self.k3)\n",
    "        self.dnn.register_parameter('k4', self.k4)\n",
    "        \n",
    "    def pred(self, c_pred):\n",
    "        self.cB = c_pred[:,0].reshape(-1,1)\n",
    "        self.cTG = c_pred[:,1].reshape(-1,1)\n",
    "        self.cMG = c_pred[:,3].reshape(-1,1)\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        self.cDG = self.dnn(g)\n",
    "        \n",
    "        grad_cDG = autograd.grad(self.cDG, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cDG_ode = self.loss_function(grad_cDG - self.k1*self.cTG + self.k2*self.cDG*self.cB \\\n",
    "                                                   + self.k3*self.cDG - self.k4*self.cMG*self.cB, f_hat)\n",
    "        \n",
    "        loss_cDG_data = self.loss_function(self.cDG, y)\n",
    "        \n",
    "        loss = alpha*loss_cDG_ode + (1-alpha)*loss_cDG_data\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        self.pred(c_pred)\n",
    "        \n",
    "        loss = self.loss(t_train, cDG_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "cDGNN().dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN Monoglycerides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cMGNN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dnn = subNN().to(device)\n",
    "        self.loss_function = nn.MSELoss(reduction = 'mean')\n",
    "        \n",
    "        self.k3 = torch.tensor([k3], requires_grad=True).float().to(device)\n",
    "        self.k4 = torch.tensor([k4], requires_grad=True).float().to(device)\n",
    "        self.k5 = torch.tensor([k5], requires_grad=True).float().to(device)\n",
    "        self.k6 = torch.tensor([k6], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.k3 = nn.Parameter(self.k3)\n",
    "        self.k4 = nn.Parameter(self.k4)\n",
    "        self.k5 = nn.Parameter(self.k5)\n",
    "        self.k6 = nn.Parameter(self.k6)\n",
    "        \n",
    "        self.dnn.register_parameter('k3', self.k3)\n",
    "        self.dnn.register_parameter('k4', self.k4)\n",
    "        self.dnn.register_parameter('k5', self.k5)\n",
    "        self.dnn.register_parameter('k6', self.k6)\n",
    "        \n",
    "    def pred(self, c_pred):\n",
    "        self.cB = c_pred[:,0].reshape(-1,1)\n",
    "        self.cDG = c_pred[:,2].reshape(-1,1)\n",
    "        self.cG = c_pred[:,4].reshape(-1,1)\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        self.cMG = self.dnn(g)\n",
    "        \n",
    "        grad_cMG = autograd.grad(self.cMG, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cMG_ode = self.loss_function(grad_cMG - self.k3*self.cDG + self.k4*self.cMG*self.cB \\\n",
    "                                                   + self.k5*self.cMG - self.k6*self.cG*self.cB, f_hat)\n",
    "        \n",
    "        loss_cMG_data = self.loss_function(self.cMG, y)\n",
    "        \n",
    "        loss = alpha*loss_cMG_ode + (1-alpha)*loss_cMG_data\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        self.pred(c_pred)\n",
    "        \n",
    "        loss = self.loss(t_train, cMG_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "cMGNN().dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN Biodiesel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cBNN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dnn = subNN().to(device)\n",
    "        self.loss_function = nn.MSELoss(reduction = 'mean')\n",
    "        \n",
    "        self.k1 = torch.tensor([k1], requires_grad=True).float().to(device)\n",
    "        self.k2 = torch.tensor([k2], requires_grad=True).float().to(device)\n",
    "        self.k3 = torch.tensor([k3], requires_grad=True).float().to(device)\n",
    "        self.k4 = torch.tensor([k4], requires_grad=True).float().to(device)\n",
    "        self.k5 = torch.tensor([k5], requires_grad=True).float().to(device)\n",
    "        self.k6 = torch.tensor([k6], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.k1 = nn.Parameter(self.k1)\n",
    "        self.k2 = nn.Parameter(self.k2)\n",
    "        self.k3 = nn.Parameter(self.k3)\n",
    "        self.k4 = nn.Parameter(self.k4)\n",
    "        self.k5 = nn.Parameter(self.k5)\n",
    "        self.k6 = nn.Parameter(self.k6)\n",
    "        \n",
    "        self.dnn.register_parameter('k1', self.k1)\n",
    "        self.dnn.register_parameter('k2', self.k2)\n",
    "        self.dnn.register_parameter('k3', self.k3)\n",
    "        self.dnn.register_parameter('k4', self.k4)\n",
    "        self.dnn.register_parameter('k5', self.k5)\n",
    "        self.dnn.register_parameter('k6', self.k6)\n",
    "        \n",
    "    def pred(self, c_pred):\n",
    "        self.cTG = c_pred[:,1].reshape(-1,1)\n",
    "        self.cDG = c_pred[:,2].reshape(-1,1)\n",
    "        self.cMG = c_pred[:,3].reshape(-1,1)\n",
    "        self.cG = c_pred[:,4].reshape(-1,1)\n",
    "        \n",
    "    def loss(self, x):\n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        self.cB = self.dnn(g)\n",
    "        \n",
    "        grad_cB = autograd.grad(self.cB, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cB_ode = self.loss_function(grad_cB - self.k1*self.cTG + self.k2*self.cDG*self.cB \\\n",
    "                                                 - self.k3*self.cDG + self.k4*self.cMG*self.cB \\\n",
    "                                                 - self.k5*self.cMG + self.k6*self.cG*self.cB, f_hat)\n",
    "        \n",
    "        return loss_cB_ode\n",
    "    \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        self.pred(c_pred)\n",
    "        \n",
    "        loss = self.loss(t_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "cBNN().dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN Glycerol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cGNN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dnn = subNN().to(device)\n",
    "        self.loss_function = nn.MSELoss(reduction = 'mean')\n",
    "        \n",
    "        self.k5 = torch.tensor([k5], requires_grad=True).float().to(device)\n",
    "        self.k6 = torch.tensor([k6], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.k5 = nn.Parameter(self.k5)\n",
    "        self.k6 = nn.Parameter(self.k6)\n",
    "        \n",
    "        self.dnn.register_parameter('k5', self.k5)\n",
    "        self.dnn.register_parameter('k6', self.k6)\n",
    "        \n",
    "    def pred(self, c_pred):\n",
    "        self.cB = c_pred[:,0].reshape(-1,1)\n",
    "        self.cMG = c_pred[:,3].reshape(-1,1)\n",
    "        \n",
    "    def loss(self, x):\n",
    "        g = x.clone()\n",
    "        g.requires_grad = True\n",
    "        \n",
    "        self.cG = self.dnn(g)\n",
    "        \n",
    "        grad_cG = autograd.grad(self.cG, g, torch.ones(x.shape[0], 1).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        loss_cG_ode = self.loss_function(grad_cG - self.k5*self.cMG + self.k6*self.cG*self.cB, f_hat)\n",
    "        \n",
    "        return loss_cG_ode\n",
    "    \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        self.pred(c_pred)\n",
    "        \n",
    "        loss = self.loss(t_train)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "cGNN().dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = []\n",
    "\n",
    "PINN_cB = cBNN()\n",
    "params = list(PINN_cB.dnn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "optimizers.append(optimizer)\n",
    "\n",
    "PINN_cTG = cTGNN()\n",
    "params = list(PINN_cTG.dnn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "optimizers.append(optimizer)\n",
    "\n",
    "PINN_cDG = cDGNN()\n",
    "params = list(PINN_cDG.dnn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "optimizers.append(optimizer)\n",
    "\n",
    "PINN_cMG = cMGNN()\n",
    "params = list(PINN_cMG.dnn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "optimizers.append(optimizer)\n",
    "\n",
    "PINN_cG = cGNN()\n",
    "params = list(PINN_cG.dnn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "epoch = 0\n",
    "max_epochs = 10\n",
    "while epoch < max_epochs:\n",
    "    # Forward\n",
    "    c_pred[:,0] = PINN_cB.dnn(t_train).detach().clone().flatten()\n",
    "    c_pred[:,1] = PINN_cTG.dnn(t_train).detach().clone().flatten()\n",
    "    c_pred[:,2] = PINN_cDG.dnn(t_train).detach().clone().flatten()\n",
    "    c_pred[:,3] = PINN_cMG.dnn(t_train).detach().clone().flatten()\n",
    "    c_pred[:,4] = PINN_cG.dnn(t_train).detach().clone().flatten()\n",
    "    \n",
    "    # Backward\n",
    "    optimizers[0].step(PINN_cB.closure)\n",
    "    for p in PINN_cB.dnn.parameters():\n",
    "        p.data.clamp_(min=0.)\n",
    "    \n",
    "    optimizers[1].step(PINN_cTG.closure)\n",
    "    for p in PINN_cTG.dnn.parameters():\n",
    "        p.data.clamp_(min=0.)\n",
    "    \n",
    "    optimizers[2].step(PINN_cDG.closure)\n",
    "    for p in PINN_cDG.dnn.parameters():\n",
    "        p.data.clamp_(min=0.)\n",
    "    \n",
    "    optimizers[3].step(PINN_cMG.closure)\n",
    "    for p in PINN_cMG.dnn.parameters():\n",
    "        p.data.clamp_(min=0.)\n",
    "    \n",
    "    optimizers[4].step(PINN_cG.closure)\n",
    "    for p in PINN_cG.dnn.parameters():\n",
    "        p.data.clamp_(min=0.)\n",
    "    \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDO(y, prm):\n",
    "    \n",
    "    k1 = prm[0]\n",
    "    k2 = prm[1]\n",
    "    k3 = prm[2]\n",
    "    k4 = prm[3]\n",
    "    k5 = prm[4]\n",
    "    k6 = prm[5]\n",
    "    \n",
    "    f = np.zeros(5)\n",
    "    \n",
    "    f[1] = - k1*y[1] + k2*y[2]*y[0]\n",
    "    f[2] = + k1*y[1] - k2*y[2]*y[0] - k3*y[2] + k4*y[3]*y[0]\n",
    "    f[3] = + k3*y[2] - k4*y[3]*y[0] - k5*y[3] + k6*y[4]*y[0]\n",
    "    f[4] = + k5*y[3] - k6*y[4]*y[0]\n",
    "    f[0] = + k1*y[1] - k2*y[2]*y[0] + k3*y[2] - k4*y[3]*y[0] + k5*y[3] - k6*y[4]*y[0]\n",
    "    \n",
    "    return f\n",
    "\n",
    "def euler_explicite(y0, dt, tf, prm):\n",
    "    \n",
    "    mat_y = np.array([y0])\n",
    "    \n",
    "    t = np.array([0])\n",
    "    while t[-1] < tf:\n",
    "        y = y0 + dt * EDO(y0, prm)\n",
    "        \n",
    "        mat_y = np.append(mat_y, [y], axis=0)\n",
    "        t = np.append(t, t[-1]+dt)\n",
    "        \n",
    "        y0 = np.copy(y)\n",
    "    \n",
    "    return t, mat_y\n",
    "\n",
    "prm = []        \n",
    "for i in range(6):\n",
    "    prm.append(params[:6][i][0].cpu().detach().numpy())\n",
    "    \n",
    "t_euler, y_euler = euler_explicite(np.array([0,0.540121748,0.057018273,0,0]), 0.001, 6, prm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,0])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[0].detach().numpy())\n",
    "plt.plot(t, cB, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,1])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[1].detach().numpy())\n",
    "plt.plot(t, cTG, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,2])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[2].detach().numpy())\n",
    "plt.plot(t, cDG, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,3])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[3].detach().numpy())\n",
    "plt.plot(t, cMG, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_euler, y_euler[:,4])\n",
    "plt.plot(t_train.detach().numpy(), PINN.dnn(t_train)[4].detach().numpy())\n",
    "plt.plot(t, cG, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f538c6a538005cf3b39e004aac3ceb405c590ed47051b2fc8a4226903742b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
